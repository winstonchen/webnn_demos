`.npy` files under this folder are downloaded from model of SSD MobileNet V1 generated by following commands, which is licensed under the Apache 2.0.

### How to Generate SSD MobileNet V1 TFLite Model

Download and extract the origin [SSD MobileNet V1 model](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz), use following converter command to remove the preprocess and postprocess graphs by using tensorflow's [`optimize_for_inference`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py) tool:

```
python3 -m tensorflow.python.tools.optimize_for_inference \
--input=./frozen_inference_graph.pb \
--output=./frozen_inference_graph_stripped.pb --frozen_graph=True \
--input_names=Preprocessor/sub \
--output_names='concat,concat_1' \
--alsologtostderr
```

Use ['TensorFlow Lite converter'](https://www.tensorflow.org/lite/convert) tool to convert frozen graph to tflite model:

```
tflite_convert \
--graph_def_file=./frozen_inference_graph_stripped.pb \
--output_file=./ssd_mobilenet_v1_coco.tflite \
--input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \
--input_shapes=1,300,300,3 --input_arrays=Preprocessor/sub \
--enable_v1_converter \
--output_arrays='concat,concat_1' \
--inference_type=FLOAT --logtostderr
```